{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59ad6a6-75c0-4688-963d-ecc4f262d418",
   "metadata": {},
   "source": [
    "# Robot Execution Failures (LP4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc837b-678b-45de-ad99-e6ed024a35e4",
   "metadata": {},
   "source": [
    "## Data Set Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc61403-4a30-41e2-8abc-09c1f3732c0f",
   "metadata": {},
   "source": [
    "### Accesing and Splitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b32fe0-7aaa-4edb-aa2b-b5cb0c7dabc5",
   "metadata": {},
   "source": [
    "The dataset is already available in the folder so please don't change the directory of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87397bf-c834-4fcb-879a-a0f87c36ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "#install the pandas and numpy library which is used for data manipulation, preparation\n",
    "#os is a built in library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0217a0-1444-471e-86ea-7ccc0bac8521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fx_1  Fy_1  Fz_1  Tx_1  Ty_1  Tz_1  Fx_2  Fy_2  Fz_2  Tx_2  ...  Fz_14  \\\n",
      "0    -2     2    20     5    -6    -1    -2     1    20     5  ...     23   \n",
      "1    -3     2    22     5    -8     0    -2     2    19     5  ...      6   \n",
      "2    -2     2    20     5    -6    -1    -2     2    19     5  ...     41   \n",
      "3    -2     2    20     5    -6    -1    -3     1    18     4  ...     32   \n",
      "4    -2     2    20     4    -7    -1    -2     1    19     5  ...      4   \n",
      "\n",
      "   Tx_14  Ty_14  Tz_14  Fx_15  Fy_15  Fz_15  Tx_15  Ty_15  Tz_15  \n",
      "0      9      2      0     -2      2     29      3     -6      0  \n",
      "1      5     -8     -1     -3      3     24      4     -8     -1  \n",
      "2      4     -5     -2     -3      1      3      5     -6      0  \n",
      "3      4    -10     -1     -2      1     30      5     -5     -1  \n",
      "4      6     -8      0     -5      4     38     -1    -16     -1  \n",
      "\n",
      "[5 rows x 90 columns]\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64\n",
      "     Fx_1  Fy_1  Fz_1  Tx_1  Ty_1  Tz_1  Fx_2  Fy_2  Fz_2  Tx_2  ...  Fz_14  \\\n",
      "112    -5    14   -43  -101   -49    17    -4     1     6    -2  ...     12   \n",
      "113    36    -4    19     8    53    -2    25     2    25     0  ...      4   \n",
      "114   -12    17     3   -19   -10    -4   -12    12    11   -13  ...      9   \n",
      "115   -41    21    -5   -23   -59    -4   -32    21    -6   -25  ...      2   \n",
      "116     9   -10   -11    17     7    -4     5     0     4     0  ...     -5   \n",
      "\n",
      "     Tx_14  Ty_14  Tz_14  Fx_15  Fy_15  Fz_15  Tx_15  Ty_15  Tz_15  \n",
      "112     -3      1     -3     -1      1      2     -2      2     -3  \n",
      "113      1      4     -3      2     -1      3      1      5     -4  \n",
      "114      0      3     -4      1      2      9      0      3     -4  \n",
      "115      2     -3     -3     -2      3      5      0     -2     -3  \n",
      "116     -1      1     -3     -1      1      4      0     -1     -3  \n",
      "\n",
      "[5 rows x 90 columns]\n",
      "112    1\n",
      "113    1\n",
      "114    1\n",
      "115    1\n",
      "116    1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#define columns\n",
    "column_groups = ['Fx', 'Fy', 'Fz', 'Tx', 'Ty', 'Tz']\n",
    "columns = [f\"{group}_{i+1}\" for i in range(15) for group in column_groups] + ['label']\n",
    "#name of the columns in the dataset\n",
    "column_types = ['continuous'] * 90 + ['categorical']\n",
    "#column types is useful to understand what each column represents in the dataset\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "data_path = os.path.join(project_root, 'dataset', 'Robot_Execution_Failures_Dataset', 'lp4.data')\n",
    "\n",
    "data = []\n",
    "current_features = []\n",
    "current_label = None\n",
    "\n",
    "with open(data_path, 'r') as f:  #open file\n",
    "    for line in f: \n",
    "        stripped = line.strip() #\n",
    "        if not stripped:\n",
    "            continue            #skip empty lines\n",
    "\n",
    "        tokens = stripped.split() #tokenize lines\n",
    "\n",
    "        # Check if line contains ANY integer (positive or negative)\n",
    "        contains_integer = any(token.lstrip('-').isdigit() for token in tokens) #boolean checks if there is no integer, means label, even a single integer means continious feature\n",
    "\n",
    "        if contains_integer:\n",
    "            # It's a feature row\n",
    "            current_features.extend(map(int, tokens))       #convert token into integer and map into an array one by one in order\n",
    "        else:\n",
    "            # It's a label\n",
    "            if current_label is not None and len(current_features) == 90:     #if the feature array has been filled, append the data, move on for the new data entry\n",
    "                data.append(current_features + [current_label])\n",
    "            current_label = stripped\n",
    "            current_features = []\n",
    "\n",
    "    # Handle last record\n",
    "    if current_label is not None and len(current_features) == 90:         #last data entry doesn't get any more label non integer values so check if the feature row has been filled if so finish the data entry\n",
    "        data.append(current_features + [current_label])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "for i, col_type in enumerate(column_types):       #Apply mapping to categorical columns\n",
    "    if col_type == 'categorical':\n",
    "        col_name = df.columns[i]\n",
    "        unique_values = df[col_name].unique()\n",
    "        numerical_values = list(range(len(unique_values)))\n",
    "        mapping = dict(zip(unique_values, numerical_values))\n",
    "        df[col_name] = df[col_name].map(mapping)\n",
    "\n",
    "column_types.pop(df.columns.get_loc('label'))   # pop unnecessary column from column types array\n",
    "input_dataset=df.drop(columns='label')          # pop unnecessary column from input dataset such as the target attribute\n",
    "output_dataset= df['label']                     #equalize output dataset to target attribute column\n",
    "preview_input_head = input_dataset.head(5).copy()\n",
    "preview_output_head = output_dataset.head(5).copy()\n",
    "preview_input_tail = input_dataset.tail(5).copy()\n",
    "preview_output_tail = output_dataset.tail(5).copy()\n",
    "print(preview_input_head)\n",
    "print(preview_output_head)\n",
    "print(preview_input_tail)\n",
    "print(preview_output_tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c932efd3-55b9-4d42-bf17-f4e6537d699e",
   "metadata": {},
   "source": [
    "### Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2dcb31-9555-40e1-bb15-1df494e1730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for NaN or incompatible data entries and replacing the missing inputs with 0 and deleting the entries that lack an output\n",
    "# Replace '?' with NaN to make it compatible with pandas handling\n",
    "input_dataset.replace('?', np.nan, inplace=True)   #replacing ? with NaN values, because below NaN values are handled, point is to convert any non value to NaN to be handled a few lines below\n",
    "\n",
    "if input_dataset.isnull().values.any():\n",
    "    input_dataset = input_dataset.fillna(0) #Filling missing inputs or NaN with 0\n",
    "valid_mask = output_dataset.isin([0, 1])\n",
    "if not valid_mask.all():\n",
    "    input_dataset = input_dataset[valid_mask].reset_index(drop=True) # dropping rows that lacks valid output\n",
    "    output_dataset = output_dataset[valid_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fdfc7a-324a-4451-b77e-7f9570e8f0e1",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "339003f2-e0af-4a4d-8d60-b8d02504dba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Fx_1      Fy_1      Fz_1      Tx_1      Ty_1      Tz_1      Fx_2  \\\n",
      "0 -0.050943 -0.033875  0.366401  0.231488 -0.015127 -0.034747 -0.042365   \n",
      "1 -0.079376 -0.033875  0.398204  0.231488 -0.050984  0.029402 -0.042365   \n",
      "2 -0.050943 -0.033875  0.366401  0.231488 -0.015127 -0.034747 -0.042365   \n",
      "3 -0.050943 -0.033875  0.366401  0.231488 -0.015127 -0.034747 -0.074900   \n",
      "4 -0.050943 -0.033875  0.366401  0.212559 -0.033056 -0.034747 -0.042365   \n",
      "5 -0.050943 -0.076110  0.350499  0.212559 -0.015127  0.029402 -0.042365   \n",
      "6 -0.022510 -0.076110  0.350499  0.212559 -0.015127  0.029402 -0.042365   \n",
      "7 -0.050943 -0.076110  0.350499  0.212559 -0.015127  0.029402 -0.042365   \n",
      "8 -0.022510 -0.033875  0.382302  0.193630 -0.015127  0.093550 -0.009829   \n",
      "9  0.005924 -0.076110  0.286892  0.193630  0.038658  0.029402 -0.042365   \n",
      "\n",
      "       Fy_2      Fz_2      Tx_2  ...     Fz_14     Tx_14     Ty_14     Tz_14  \\\n",
      "0 -0.036106  0.350786  0.316542  ...  0.672068  0.335207  0.150229  0.462562   \n",
      "1  0.026915  0.336934  0.316542  ...  0.380207  0.103697 -0.381946  0.185025   \n",
      "2  0.026915  0.336934  0.316542  ...  0.981098  0.045820 -0.222294 -0.092512   \n",
      "3 -0.036106  0.323081  0.282589  ...  0.826583  0.045820 -0.488381  0.185025   \n",
      "4 -0.036106  0.336934  0.316542  ...  0.345870  0.161575 -0.381946  0.462562   \n",
      "5 -0.036106  0.336934  0.282589  ...  0.517553 -0.012058 -0.169076  0.462562   \n",
      "6  0.026915  0.323081  0.282589  ...  0.517553  0.103697 -0.115859  0.740100   \n",
      "7 -0.036106  0.323081  0.248636  ...  0.843751  0.161575 -0.275511  0.462562   \n",
      "8  0.026915  0.309229  0.248636  ...  0.569058 -0.069935 -0.488381  0.740100   \n",
      "9 -0.036106  0.517017  0.248636  ...  0.260028 -0.012058 -0.169076  0.462562   \n",
      "\n",
      "      Fx_15     Fy_15     Fz_15     Tx_15     Ty_15     Tz_15  \n",
      "0 -0.180863  0.424114  0.570400  0.009107 -0.455277  0.320806  \n",
      "1 -0.359862  0.585042  0.517636  0.060533 -0.567778  0.190859  \n",
      "2 -0.359862  0.263185  0.296027  0.111958 -0.455277  0.320806  \n",
      "3 -0.180863  0.263185  0.580952  0.111958 -0.399027  0.190859  \n",
      "4 -0.717860  0.745971  0.665375 -0.196597 -1.017783  0.190859  \n",
      "5  0.177134  0.102257  0.507083  0.163384 -0.286526  0.190859  \n",
      "6  0.356133  0.102257  0.517636  0.420514 -0.230275  0.320806  \n",
      "7 -0.180863  0.263185  0.433213  0.111958 -0.624029  0.320806  \n",
      "8  0.535132  0.102257  0.443766  0.266236 -0.174025  0.320806  \n",
      "9  0.356133  0.102257  0.528188 -0.145171 -0.286526  0.060913  \n",
      "\n",
      "[10 rows x 90 columns]\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "9    0\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Now instead of MinMax Scaler I would like to use NormalScaler without a library which is fairly easy\n",
    "for idx, col_type in enumerate(column_types):   #iterate through columns \n",
    "    if col_type == 'continuous':                 #apply normalization only to continous type of columns\n",
    "        col_name = input_dataset.columns[idx]    \n",
    "        mean = input_dataset[col_name].mean()    #get the mean of the col\n",
    "        std = input_dataset[col_name].std()      #get the std of the col\n",
    "        input_dataset[col_name] = (input_dataset[col_name] - mean) / (std + 1e-18) #Z Score Normalization - very small epsilon to prevent divide by zero error\n",
    "\n",
    "preview_input = input_dataset.head(10).copy()\n",
    "preview_output = output_dataset.head(10).copy()\n",
    "print(preview_input)\n",
    "print(preview_output) \n",
    "#let's see how the datasets have been prepared for further ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d0d7ed-8692-4be6-ad5c-e1cbaf42ec40",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19fde261-b03f-4171-bf45-70d7aa77fb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0     1     2     3     4     5     6     7     8     9   ...    80  \\\n",
      "0  0.46  0.56  0.54 -0.36  1.33 -1.70  0.41  0.41  0.23 -0.84  ...  0.14   \n",
      "1 -0.82 -0.37  0.11  0.29 -0.48 -0.10 -0.76  0.22 -0.01 -0.09  ...  0.31   \n",
      "2  0.49 -1.64 -0.41  0.06  0.49 -4.20 -0.30  0.28  0.38  0.52  ...  0.35   \n",
      "3  0.26 -0.54 -0.13  0.46  0.22 -0.23  0.19 -0.10  0.13  0.15  ...  0.19   \n",
      "4  0.03 -0.12  0.22  0.12  0.25  0.09  0.02 -0.10  0.02  0.01  ... -0.07   \n",
      "5  1.14 -0.67  0.48  0.63  1.08  0.09  1.06 -0.35  0.50  0.32  ...  0.59   \n",
      "6 -0.05 -0.03  0.37  0.23 -0.02 -0.03 -0.07 -0.04  0.32  0.28  ...  0.83   \n",
      "7 -0.39  1.11 -0.27 -0.79  0.09 -0.55 -0.82 -1.74  0.16  1.74  ...  0.19   \n",
      "8  0.09 -0.12 -0.09  0.10  0.13  0.09  0.15 -0.04 -0.26  0.01  ... -2.04   \n",
      "9 -0.11 -0.08  0.16  0.23  0.06  0.16 -0.11 -0.10  0.17  0.32  ...  0.31   \n",
      "\n",
      "     81    82    83    84    85    86    87    88    89  \n",
      "0 -0.76 -0.75 -0.65  0.18  0.26  0.32 -0.51 -0.62 -0.07  \n",
      "1 -0.36  0.20 -0.65 -0.00 -0.06  0.16 -0.15  0.22 -0.20  \n",
      "2  0.10  0.20  0.19 -0.00 -0.06  0.20  0.01  0.11  0.19  \n",
      "3 -0.24  0.10 -0.37 -0.00  0.26  0.31 -0.15 -0.17 -0.07  \n",
      "4 -1.75 -0.49  0.74  0.18 -0.06  0.21 -0.66  0.61  0.45  \n",
      "5  0.28 -8.10 -1.76 -0.54  0.26  0.36 -0.51 -0.29  0.19  \n",
      "6  0.05 -0.49  0.19 -0.18  0.26  0.58  0.11 -0.40  0.19  \n",
      "7  0.68  0.31 -0.37  0.18  0.59  0.15 -0.25  0.16 -0.07  \n",
      "8  0.74  0.58  1.30  1.25 -1.67 -1.17  0.73  0.44  0.71  \n",
      "9  0.05 -0.01  1.02 -0.36  0.59  0.45 -0.04 -0.06  0.32  \n",
      "\n",
      "[10 rows x 90 columns]\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "5    1\n",
      "6    0\n",
      "7    1\n",
      "8    1\n",
      "9    0\n",
      "dtype: int64\n",
      "     0     1     2     3     4     5     6     7     8     9   ...    80  \\\n",
      "0 -1.30  2.67 -0.29 -2.04 -0.10 -0.03 -2.29  0.09  0.18 -0.36  ...  0.50   \n",
      "1 -1.61 -0.29  0.68  0.35 -2.70 -2.66 -0.56  0.41  0.23  0.04  ...  0.29   \n",
      "2  0.35 -0.37  0.45  0.38  0.40 -0.03  0.35 -0.23  0.21  0.25  ...  0.45   \n",
      "3  0.43 -0.67 -0.02  0.88 -0.46  5.42 -0.43  0.09  0.09  0.01  ...  0.48   \n",
      "4  0.55  1.15  0.75 -2.08  0.76  2.60 -1.02  0.34  0.39 -0.09  ...  0.05   \n",
      "5  0.09 -0.16 -0.06  0.27  0.20  0.35  0.12  0.15 -0.55  0.11  ...  0.07   \n",
      "6 -0.11 -0.12  0.18  0.23  0.07  0.16 -0.11 -0.10  0.17  0.32  ...  0.65   \n",
      "7 -0.11  0.30 -0.27 -0.17  0.25  0.16 -0.69  1.10  0.21 -1.42  ... -2.11   \n",
      "8  0.63 -0.08  0.24  0.21  0.58 -0.16  0.64 -0.23  0.28  0.45  ...  0.53   \n",
      "9  0.01 -0.08  0.29  0.19  0.04  0.03 -0.04 -0.04  0.52  0.25  ...  0.26   \n",
      "\n",
      "     81    82    83    84    85    86    87    88    89  \n",
      "0 -0.13  0.04 -0.09 -0.00  0.26  0.46  0.21  0.11 -0.07  \n",
      "1 -0.42  0.26 -0.37 -0.00  0.10  0.17 -0.30  0.22 -0.20  \n",
      "2 -0.13 -0.06  0.19 -0.36  0.10  0.13 -0.20 -0.40  0.19  \n",
      "3 -0.48  0.31 -0.09 -0.18 -0.06  0.38 -0.45  0.16  0.06  \n",
      "4 -1.11 -0.75 -0.93  0.54  0.26  0.25 -0.92 -1.13 -0.20  \n",
      "5 -0.65 -0.54  1.02 -0.54  0.10  0.20 -0.25 -0.23  0.58  \n",
      "6  0.22  0.10  0.46 -0.54  0.26  0.20 -0.04 -0.23  0.45  \n",
      "7  1.32  0.84 -1.48  1.07 -2.79 -1.19  1.19  0.78 -0.59  \n",
      "8 -0.01 -0.06 -0.37  0.18  0.26  0.31 -0.09 -0.17 -0.07  \n",
      "9 -0.01 -0.17  0.46  0.36  0.10  0.53 -0.15 -0.29  0.06  \n",
      "\n",
      "[10 rows x 90 columns]\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "5    1\n",
      "6    0\n",
      "7    1\n",
      "8    1\n",
      "9    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Split the dataset for 80-20 ratio, use 80% for training and 20% for testing, kind of like rule of thumb in ML.\n",
    "combined = list(zip(input_dataset.values.tolist(), output_dataset))\n",
    "np.random.seed(53)\n",
    "np.random.shuffle(combined)\n",
    "\n",
    "input_shuffled, output_shuffled = zip(*combined)\n",
    "input_shuffled = np.array(input_shuffled)\n",
    "output_shuffled = np.array(output_shuffled)\n",
    "split_point = int(0.8 * len(input_shuffled))\n",
    "input_shuffled_train_dataset, input_shuffled_test_dataset = input_shuffled[:split_point], input_shuffled[split_point:]\n",
    "output_shuffled_train_dataset, output_shuffled_test_dataset = output_shuffled[:split_point], output_shuffled[split_point:]\n",
    "\n",
    "\n",
    "preview_input_train = input_shuffled_train_dataset[:10]\n",
    "preview_output_train = output_shuffled_train_dataset[:10]\n",
    "preview_input_test = input_shuffled_test_dataset[:10]\n",
    "preview_output_test = output_shuffled_test_dataset[:10]\n",
    "\n",
    "print(pd.DataFrame(preview_input_train).round(2))\n",
    "print(pd.Series(preview_output_train))\n",
    "print(pd.DataFrame(preview_input_test).round(2))\n",
    "print(pd.Series(preview_output_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2017b9b-08f4-4b2d-82af-6e920d7f66ba",
   "metadata": {},
   "source": [
    "## ML Models Implementation and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b927b837-7f5b-4468-8893-3654f8f5ab53",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b40fde60-f8e3-448a-b370-75bd904f3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.001, iter=1000, features=None, target_attribute=None):\n",
    "        self.learning_rate = learning_rate   #coffecient for how hard the weights and bias is updated according to the gradient descent\n",
    "        self.iter = iter                     #number of iterations until the model is complete and weights and bias is updated fully\n",
    "        self.features = features             #input dataset\n",
    "        self.target_attribute = target_attribute  #output dataset\n",
    "        self.weights = None              \n",
    "        self.bias = None\n",
    "\n",
    "    def train(self):\n",
    "        num_samples = self.features.shape[0]\n",
    "        num_features = self.features.shape[1]\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0.0\n",
    "\n",
    "        for _ in range(self.iter):\n",
    "            predicted_output = np.dot(self.features, self.weights) + self.bias #X.w+b\n",
    "            error = predicted_output - self.target_attribute\n",
    "            dw = np.mean(self.features * error[:, np.newaxis], axis=0)  #gradient descent of w\n",
    "            db = np.mean(error)                                         #gradient descent of bias \n",
    "            self.weights -= self.learning_rate * dw                      #updating weight with lr\n",
    "            self.bias -= self.learning_rate * db                        #updating bias with lr\n",
    "\n",
    "    def predict(self, input_test):\n",
    "        return np.dot(input_test, self.weights) + self.bias\n",
    "\n",
    "    def calculate_mse(self, input_test, actual_output):\n",
    "        predicted_output = self.predict(input_test)\n",
    "        mse = np.mean((predicted_output - actual_output) ** 2)\n",
    "        return mse\n",
    "        \n",
    "    def calculate_efficiency(self, input_test, output_test):\n",
    "        output_pred = self.predict(input_test)\n",
    "        output_pred_rounded= np.round(output_pred).astype(int)\n",
    "        return np.mean(output_pred_rounded == output_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "524cc818-9a1c-40ca-a6d9-585d00b3f925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Classification MSE: 0.70\n",
      "Rounded Linear Regression Classification Accuracy: 30.00%\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(features=input_shuffled_train_dataset, target_attribute=output_shuffled_train_dataset)\n",
    "model.train()\n",
    "linear_regression_model_mse=model.calculate_mse(input_shuffled_test_dataset, output_shuffled_test_dataset)\n",
    "linear_regression_model_accuracy= model.calculate_efficiency(input_shuffled_test_dataset,output_shuffled_test_dataset)\n",
    "print(f\"Linear Regression Classification MSE: {linear_regression_model_mse :.2f}\")\n",
    "print(f\"Rounded Linear Regression Classification Accuracy: {linear_regression_model_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9738f2a-c5a6-4878-b5fc-ebd3752a9a34",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7418270-7151-4901-b559-758476df7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression is used because the output is categorized not a continous variable so no linear regression was used. \n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, iter=2000, features = None, target_attribute=None):\n",
    "        self.learning_rate = learning_rate    #coffecient for how hard the weights and bias is updated according to the gradient descent\n",
    "        self.iter = iter                       #number of iterations until the model is complete and weights and bias is updated fully\n",
    "        self.features = features               #input dataset\n",
    "        self.target_attribute = target_attribute  #output dataset\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def sigmoid_function(self, x):               #logistic regression function which is actually being applied to linearly regressed function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def train(self):                              #initializing the function variables\n",
    "        num_samples = self.features.shape[0]\n",
    "        num_features = self.features.shape[1]\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0.0\n",
    "        \n",
    "        for _ in range(self.iter):\n",
    "            z = np.dot(self.features, self.weights) + self.bias       #X.w+b\n",
    "            y_hat = self.sigmoid_function(z)                          #logistic regression function applied to linear regression function\n",
    "            \n",
    "            error = (y_hat - self.target_attribute)                   \n",
    "            dw = np.mean(self.features * error[:, np.newaxis], axis=0)  #gradient descent of w \n",
    "            db = np.mean(error)                                          #gradient descent of bias\n",
    "            \n",
    "            self.weights -= self.learning_rate * dw                        #updating weight with lr\n",
    "            self.bias -= self.learning_rate * db                           #updating bias with lr\n",
    "\n",
    "    def predict(self, input_test):  # Rename to match usage\n",
    "        z = np.dot(input_test, self.weights) + self.bias\n",
    "        probs = self.sigmoid_function(z)\n",
    "        return (probs >= 0.5).astype(int)\n",
    "\n",
    "    def calculate_efficiency(self, input_test, output_test):\n",
    "        output_pred = self.predict(input_test)\n",
    "        return np.mean(output_pred == output_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18aa422d-a052-4086-8d5a-6ff90e1f530e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 75.00%\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(features = input_shuffled_train_dataset, target_attribute=output_shuffled_train_dataset)\n",
    "model.train()\n",
    "logistic_regression_model_accuracy=model.calculate_efficiency(input_shuffled_test_dataset,output_shuffled_test_dataset)\n",
    "print(f\"Logistic Regression Accuracy: {logistic_regression_model_accuracy * 100:.2f}%\")\n",
    "# Predict the test set values and compute the accuracy of the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a44bfa2-43c7-45e2-a8ed-1940cbcfe0f4",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1a9c84-b87a-4537-b1a4-6e222f65a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Initialize dictionaries to store class probabilities as well as the variance relative to the mean\n",
    "class BayessianClassifier:\n",
    "    def __init__(self, features, target_attribute):\n",
    "        self.features = features             #input dataset\n",
    "        self.target_attribute = target_attribute               #output_dataset\n",
    "        self.available_classes = np.unique(self.target_attribute)    #possible output classes\n",
    "        \n",
    "        self.mean = {}            #mean array for columns\n",
    "        self.var = {}             #var array for columns\n",
    "        self.class_probabilities={}    #occurence ratio of output classes\n",
    "        \n",
    "    def get_probabilistic_values(self):\n",
    "         # Calculate the probability, mean and variance\n",
    "        total_samples = len(self.target_attribute)\n",
    "        for cls in self.available_classes:\n",
    "            # Select the data subset\n",
    "            input_occurence= self.features[self.target_attribute== cls]\n",
    "            self.class_probabilities[cls]=len(input_occurence)/total_samples\n",
    "            self.mean[cls] = np.mean(input_occurence, axis=0)    #calculate array of mean of columns\n",
    "            self.var[cls] = np.var(input_occurence, axis=0)      #calculate array of var of columns \n",
    "\n",
    "    def predict(self, data_entry):\n",
    "        log_posterior_array=[]\n",
    "        for cls in self.available_classes:\n",
    "            log_likelihood=0           #init lof likelihood\n",
    "            for i, col in enumerate(data_entry):\n",
    "                log_likelihood=log_likelihood+((-((col-self.mean[cls][i]) ** 2))/(2*self.var[cls][i]+1e-6))+np.log(1/(np.sqrt(2*np.pi*self.var[cls][i]+1e-6))) #gaussian pdf function, apply it to each feature and sum it up to obtain log likelihood\n",
    "            log_posterior = log_likelihood + np.log(self.class_probabilities[cls])   #log posterio = log likelihood + log prior\n",
    "            log_posterior_array.append(log_posterior)                                #append the predict into log_posterior array\n",
    "        return self.available_classes[np.argmax(log_posterior_array)]                #return the highest probability class which is the predicted output for the input data entry\n",
    "    \n",
    "\n",
    "    def calculate_efficiency(self, input_test, output_test):\n",
    "        output_pred = np.array([self.predict(sample) for sample in input_test])\n",
    "        return np.mean(output_pred == output_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "673935e8-3e6e-44d5-8165-d36a2081b82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayessian Classifier Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Train the Gaussian Naive Bayes classifier\n",
    "model = BayessianClassifier(features=input_shuffled_train_dataset, target_attribute=output_shuffled_train_dataset)\n",
    "model.get_probabilistic_values()\n",
    "bayessian_classifier_model_accuracy=model.calculate_efficiency(input_shuffled_test_dataset, output_shuffled_test_dataset)\n",
    "print(f\"Bayessian Classifier Accuracy: {bayessian_classifier_model_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44075a18-01cd-43a4-85ad-fc20b2d323e1",
   "metadata": {},
   "source": [
    "### KNearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "050e85c9-0636-468b-99a9-b7629a445132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_if_closer(neighbors, candidate):    ###get the closest neighbor array which has k elements in it. Returns the maximum distance element in the neighbors array, compare it with the candidate element, replace if candidate element is closer\n",
    "    \"\"\"\n",
    "    neighbors: list of (label, distance)\n",
    "    candidate: a new tuple (label, distance)\n",
    "    \"\"\"\n",
    "    max_dist = -float('inf')\n",
    "    max_index = -1\n",
    "\n",
    "    for i in range(len(neighbors)):         #get's the farthest element in neighbors\n",
    "        if neighbors[i][1] > max_dist:\n",
    "            max_dist = neighbors[i][1]\n",
    "            max_index = i\n",
    "    if candidate[1] < max_dist:        #replace the candidate with the farthest if it is closer\n",
    "        neighbors[max_index] = candidate\n",
    "    \n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=3, features=None, target_attribute=None):\n",
    "        self.k = k\n",
    "        self.features = features\n",
    "        self.target_attribute = target_attribute\n",
    "        self.available_classes = np.unique(self.target_attribute)    #possible output classes\n",
    "\n",
    "    def get_closest_neighbours(self, data_entry):\n",
    "        closest_neighbours = [(None, float('inf')) for _ in range(self.k)]         #start with dummy neighbors with infinite distance\n",
    "\n",
    "        for i in range(len(self.features)):                                        #calculate the euclidean distance of the candidate data entry\n",
    "            distance = np.sqrt(np.sum((self.features[i] - data_entry) ** 2))\n",
    "            label = self.target_attribute[i]\n",
    "            candidate = (label, distance)\n",
    "\n",
    "            replace_if_closer(closest_neighbours, candidate)                        #replace if the candidate data entry is closer\n",
    "        return closest_neighbours\n",
    "\n",
    "    def predict(self, closest_neighbours):\n",
    "        label_counts = {label: 0 for label in self.available_classes}\n",
    "        for label, _ in closest_neighbours:\n",
    "            label_counts[label] += 1\n",
    "    \n",
    "        max_count = -1\n",
    "        predicted_label = None\n",
    "    \n",
    "        for label in label_counts:\n",
    "            if label_counts[label] > max_count:\n",
    "                max_count = label_counts[label]\n",
    "                predicted_label = label\n",
    "    \n",
    "        return predicted_label\n",
    "    \n",
    "    def calculate_efficiency(self, input_test, output_test):\n",
    "        output_pred = np.array([self.predict(self.get_closest_neighbours(sample)) for sample in input_test]) #get output pred array by predicting each entry with corresponding closest neighbours to each set in input dataset\n",
    "        return np.mean(output_pred == output_test)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16a03fdb-4d90-4583-8785-b2e1c57ddbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier Accuracy with k=1: 85.00%\n",
      "KNN Classifier Accuracy with k=2: 80.00%\n",
      "KNN Classifier Accuracy with k=3: 80.00%\n",
      "KNN Classifier Accuracy with k=4: 80.00%\n",
      "KNN Classifier Accuracy with k=5: 80.00%\n",
      "KNN Classifier Accuracy with k=6: 75.00%\n",
      "KNN Classifier Accuracy with k=7: 80.00%\n",
      "\n",
      "Best k: 1 with Accuracy: 85.00%\n"
     ]
    }
   ],
   "source": [
    "best_k = None\n",
    "KNN_classifier_model_accuracy = 0.0\n",
    "\n",
    "for i in range(1, 8):\n",
    "    model = KNNClassifier(features=input_shuffled_train_dataset, target_attribute=output_shuffled_train_dataset, k=i)\n",
    "    model_accuracy = model.calculate_efficiency(input_shuffled_test_dataset, output_shuffled_test_dataset)\n",
    "    print(f\"KNN Classifier Accuracy with k={i}: {model_accuracy * 100:.2f}%\")\n",
    "\n",
    "    if model_accuracy > KNN_classifier_model_accuracy:\n",
    "        KNN_classifier_model_accuracy = model_accuracy\n",
    "        best_k = i\n",
    "\n",
    "print(f\"\\nBest k: {best_k} with Accuracy: {KNN_classifier_model_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2b4331-4659-47d9-8c35-0d59f08aa97d",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74b525a0-22c0-48eb-b8bc-f40d18a04c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, features, target_attribute, column_types, max_depth):\n",
    "        self.depth=None\n",
    "        self.features = features\n",
    "        self.target_attribute = target_attribute\n",
    "        self.column_types = column_types\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "        if features is not None and column_types is not None:\n",
    "            if len(column_types) != features.shape[1]:\n",
    "                raise ValueError(\"Length of column_types must match number of feature columns\")\n",
    "\n",
    "    def calculate_entropy(self, output_column):\n",
    "        values, counts = np.unique(output_column, return_counts=True) #get the unique class labels from output column with their corresponding occurence frequency.\n",
    "        probabilities = counts / len(output_column) #array of probabilities by dividing occurence array by the number of rows. \n",
    "        entropy = -np.sum([p * np.log2(p + 1e-9) for p in probabilities]) #calculate the given samples entropy\n",
    "        return entropy\n",
    "\n",
    "    def categorical_split_entropy(self, feature_column, output_column):\n",
    "        unique_values = np.unique(feature_column)\n",
    "        total_entropy = 0\n",
    "        for val in unique_values:\n",
    "            mask = feature_column == val #boolean array of where the categorical value selected inside the feature column for the current iteration, return true if matches in array false if it doesn't match\n",
    "            y_sub = output_column[mask] # clipping the output column by only showing True returned booleans from the mask\n",
    "            entropy = self.calculate_entropy(y_sub) # calculate the entropy for the specific attribute chosen for this iteration in the specific feature chosen in this function\n",
    "            weight = len(y_sub) / len(output_column) #calculate the weight of the specific attribute for this iteration\n",
    "            total_entropy += weight * entropy #multiply specific attribute of the specific feature with its weight, iterate through each attribute for the specific feature eventually obtain the entropy for that specific feature column, will be used later for comparison\n",
    "        return total_entropy\n",
    "\n",
    "    def numerical_split_entropy(self, feature_column, output_column):\n",
    "        sorted_indices = np.argsort(feature_column) #sort the indices chosen feature columns numerical attributes\n",
    "        feature_column = feature_column[sorted_indices] #sort the feature column according to the sorted indices above\n",
    "        output_column = output_column[sorted_indices]  #sort the output column according to the sorted indices above\n",
    "    \n",
    "        best_entropy = float('inf')\n",
    "        best_threshold = None\n",
    "        base_entropy = self.calculate_entropy(output_column) #get the parent entropy\n",
    "    \n",
    "        for i in range(len(feature_column) - 1):\n",
    "            threshold = (feature_column[i] + feature_column[i + 1]) / 2\n",
    "    \n",
    "            left_mask = feature_column <= threshold #getting the mask boolean array into two categories, numerical attribute lower than threshold and numerical attribute greater than threshold\n",
    "            right_mask = feature_column > threshold\n",
    "    \n",
    "            left_output = output_column[left_mask] #getting the array itself according to the boolean array created above\n",
    "            right_output = output_column[right_mask]\n",
    "    \n",
    "            if len(left_output) == 0 or len(right_output) == 0: #skip these iterations if there are duplicate data right at the border and split simply split data into 0 and remaining, meaning split didn't actually split anything at all\n",
    "                continue\n",
    "    \n",
    "            left_entropy = self.calculate_entropy(left_output) #calculate entropy for left side\n",
    "            right_entropy = self.calculate_entropy(right_output) #calculate entropy for right side\n",
    "    \n",
    "            weighted_entropy = (len(left_output) * left_entropy + len(right_output) * right_entropy) / len(output_column) #get the split weighted entropy sum\n",
    "    \n",
    "            if weighted_entropy < best_entropy: #if split weighted entropy sum is lower than the best one so far, this is the best one and this splti poitn is the best split point\n",
    "                best_entropy = weighted_entropy\n",
    "                best_threshold = threshold\n",
    "    \n",
    "        info_gain = base_entropy - best_entropy #calculate info gain according to the best weighted split entropy  \n",
    "        return best_entropy, best_threshold, info_gain\n",
    "\n",
    "    def find_best_split(self, features=None, target=None, depth=0):\n",
    "        if features is None:\n",
    "            features = self.features\n",
    "        if target is None:\n",
    "            target = self.target_attribute\n",
    "\n",
    "        if len(np.unique(target)) == 1: #if the split has only one possible output, conclude that leaf with the given output\n",
    "            return {'label': target[0]}\n",
    "\n",
    "        if len(target) <= 5:   #if the split has fewer rows than 5, take the maximum occurence of the output class as reference and conclude the leaf\n",
    "            return {'label': np.bincount(target).argmax()}\n",
    "\n",
    "        if self.max_depth is not None and depth >= self.max_depth:   #if the tree depth \n",
    "            return {'label': np.bincount(target).argmax()}\n",
    "\n",
    "        base_entropy = self.calculate_entropy(target)\n",
    "        best_gain = -float('inf')\n",
    "        best_column = None\n",
    "        best_type = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for col_index in range(features.shape[1]):\n",
    "            feature_col = features[:, col_index] #extract the col\n",
    "            col_type = self.column_types[col_index] #get the col type\n",
    "\n",
    "            if col_type == 'categorical': #if the col is categorical, apply categorical entropy calculation\n",
    "                entropy = self.categorical_split_entropy(feature_col, target)\n",
    "                threshold = None\n",
    "                info_gain = base_entropy - entropy\n",
    "            else:                          #if the col is numerical, apply numerical entropy calculation\n",
    "                entropy, threshold, info_gain = self.numerical_split_entropy(feature_col, target)\n",
    "\n",
    "            if info_gain > best_gain: #get the best info gain\n",
    "                best_gain = info_gain\n",
    "                best_column = col_index\n",
    "                best_type = col_type\n",
    "                best_threshold = threshold\n",
    "\n",
    "        return best_column, best_type, best_threshold, best_gain\n",
    "\n",
    "    def build_tree(self, features=None, target=None, depth=0):\n",
    "        if features is None:\n",
    "            features = self.features\n",
    "        if target is None:\n",
    "            target = self.target_attribute\n",
    "\n",
    "        split = self.find_best_split(features, target, depth)\n",
    "        if isinstance(split, dict) and 'label' in split: #looks if split returned a leaf node or a split\n",
    "            return split\n",
    "\n",
    "        best_col, col_type, threshold, _ = split\n",
    "        feature_col = features[:, best_col]\n",
    "\n",
    "        if col_type == 'categorical':        #if the column is categorical\n",
    "            branches = {}\n",
    "            for val in np.unique(feature_col):\n",
    "                mask = feature_col == val\n",
    "                sub_features = features[mask]\n",
    "                sub_target = target[mask]\n",
    "                branches[val] = self.build_tree(sub_features, sub_target, depth + 1) #builds the tree with categorical seperation\n",
    "            return {\n",
    "                'feature_index': best_col,\n",
    "                'type': col_type,\n",
    "                'branches': branches\n",
    "            }\n",
    "        else:                                                         #else the best split decision is numerical\n",
    "            left_mask = feature_col <= threshold\n",
    "            right_mask = feature_col > threshold\n",
    "\n",
    "            left_features, left_target = features[left_mask], target[left_mask]     #left side of the threshold of the tree      \n",
    "            right_features, right_target = features[right_mask], target[right_mask]  #right side of the threshold of the tree\n",
    "            return {\n",
    "                'feature_index': best_col,\n",
    "                'type': col_type,\n",
    "                'threshold': threshold,\n",
    "                'left': self.build_tree(left_features, left_target, depth + 1),\n",
    "                'right': self.build_tree(right_features, right_target, depth + 1)\n",
    "            }\n",
    "\n",
    "    def train(self):\n",
    "        self.tree = self.build_tree()\n",
    "        self.depth = self.get_tree_depth(self.tree)  \n",
    "\n",
    "    def get_tree_depth(self, node):\n",
    "        if 'label' in node:\n",
    "            return 0\n",
    "        if node['type'] == 'categorical':\n",
    "            return 1 + max(self.get_tree_depth(branch) for branch in node['branches'].values())\n",
    "        else:\n",
    "            return 1 + max(self.get_tree_depth(node['left']), self.get_tree_depth(node['right']))\n",
    "\n",
    "\n",
    "    def predict_one(self, input_row, tree):\n",
    "        if 'label' in tree:\n",
    "            return tree['label']\n",
    "\n",
    "        if tree['type'] == 'categorical':\n",
    "            val = input_row[tree['feature_index']]\n",
    "            if val in tree['branches']:\n",
    "                return self.predict_one(input_row, tree['branches'][val])\n",
    "            else:\n",
    "                return list(tree['branches'].values())[0]['label']   #recursive call for getting down the tree\n",
    "        else:\n",
    "            if input_row[tree['feature_index']] <= tree['threshold']:\n",
    "                return self.predict_one(input_row, tree['left'])      #recursive call for getting down the tree\n",
    "            else:\n",
    "                return self.predict_one(input_row, tree['right'])      #recursive call for getting down the tree\n",
    "\n",
    "    def predict(self, input_test):\n",
    "        return np.array([self.predict_one(row, self.tree) for row in input_test])\n",
    "\n",
    "    def calculate_efficiency(self, test_input, test_output):\n",
    "        predictions = self.predict(test_input)\n",
    "        accuracy = np.mean(predictions == test_output)\n",
    "        print(f\"Decision Tree Classification Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab409824-6a8b-4468-a56c-c62f25206a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth Tried: 1, Actual Depth: 1, Accuracy: 70.00%\n",
      "Max Depth Tried: 2, Actual Depth: 2, Accuracy: 80.00%\n",
      "Max Depth Tried: 3, Actual Depth: 3, Accuracy: 85.00%\n",
      "Max Depth Tried: 4, Actual Depth: 3, Accuracy: 85.00%\n",
      "\n",
      "Max Possible Depth is: 3\n",
      "\n",
      "Best Depth: 3 with Accuracy: 85.00%\n"
     ]
    }
   ],
   "source": [
    "decision_tree_model_accuracy = 0.0\n",
    "best_depth = 1\n",
    "depth = 1\n",
    "max_possible_depth=None\n",
    "\n",
    "while True:\n",
    "    model = DecisionTree(features=input_shuffled_train_dataset, target_attribute=output_shuffled_train_dataset, column_types=column_types, max_depth=depth)\n",
    "    model.train()\n",
    "    predictions = model.predict(input_shuffled_test_dataset)\n",
    "    accuracy = np.mean(predictions == output_shuffled_test_dataset)\n",
    "    print(f\"Max Depth Tried: {depth}, Actual Depth: {model.depth}, Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    if accuracy > decision_tree_model_accuracy:\n",
    "        decision_tree_model_accuracy = accuracy\n",
    "        best_depth = depth\n",
    "\n",
    "    # if increasing max depth no longer increases actual tree depth, then it is the maximum splitted tree, break\n",
    "    if model.depth < depth:\n",
    "        max_possible_depth=model.depth\n",
    "        break\n",
    "    depth += 1\n",
    "\n",
    "print(f\"\\nMax Possible Depth is: {max_possible_depth}\")\n",
    "print(f\"\\nBest Depth: {best_depth} with Accuracy: {decision_tree_model_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4919ff7c-d8ba-44a8-a083-c59d2c37f621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Classification MSE: 0.70\n",
      "Rounded Linear Regression Classification Accuracy: 30.00%\n",
      "Logistic Regression Accuracy: 75.00%\n",
      "Decision Tree Classification Accuracy: 85.00%\n",
      "KNN Classification Accuracy: 85.00%\n",
      "Bayessian Classifier Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Linear Regression Classification MSE: {linear_regression_model_mse :.2f}\")\n",
    "print(f\"Rounded Linear Regression Classification Accuracy: {linear_regression_model_accuracy * 100:.2f}%\")\n",
    "print(f\"Logistic Regression Accuracy: {logistic_regression_model_accuracy * 100:.2f}%\")\n",
    "print(f\"Decision Tree Classification Accuracy: {decision_tree_model_accuracy * 100:.2f}%\")\n",
    "print(f\"KNN Classification Accuracy: {KNN_classifier_model_accuracy * 100:.2f}%\")\n",
    "print(f\"Bayessian Classifier Accuracy: {bayessian_classifier_model_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
