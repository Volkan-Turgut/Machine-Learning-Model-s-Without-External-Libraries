{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59ad6a6-75c0-4688-963d-ecc4f262d418",
   "metadata": {},
   "source": [
    "# Breast Cancer Wisconsin (Diagnostic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc837b-678b-45de-ad99-e6ed024a35e4",
   "metadata": {},
   "source": [
    "## Data Set Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc61403-4a30-41e2-8abc-09c1f3732c0f",
   "metadata": {},
   "source": [
    "### Accesing and Splitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b32fe0-7aaa-4edb-aa2b-b5cb0c7dabc5",
   "metadata": {},
   "source": [
    "The dataset is already available in the folder so please don't change the directory of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87397bf-c834-4fcb-879a-a0f87c36ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "#install the pandas library which is used for data manipulation, preparation\n",
    "#os is a built in library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0217a0-1444-471e-86ea-7ccc0bac8521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "5        12.45         15.70           82.57      477.1          0.12780   \n",
      "6        18.25         19.98          119.60     1040.0          0.09463   \n",
      "7        13.71         20.83           90.20      577.9          0.11890   \n",
      "8        13.00         21.82           87.50      519.8          0.12730   \n",
      "9        12.46         24.04           83.97      475.9          0.11860   \n",
      "\n",
      "   compactness_mean  concavity_mean  concave_points_mean  symmetry_mean  \\\n",
      "0           0.27760         0.30010              0.14710         0.2419   \n",
      "1           0.07864         0.08690              0.07017         0.1812   \n",
      "2           0.15990         0.19740              0.12790         0.2069   \n",
      "3           0.28390         0.24140              0.10520         0.2597   \n",
      "4           0.13280         0.19800              0.10430         0.1809   \n",
      "5           0.17000         0.15780              0.08089         0.2087   \n",
      "6           0.10900         0.11270              0.07400         0.1794   \n",
      "7           0.16450         0.09366              0.05985         0.2196   \n",
      "8           0.19320         0.18590              0.09353         0.2350   \n",
      "9           0.23960         0.22730              0.08543         0.2030   \n",
      "\n",
      "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
      "0                 0.07871  ...         25.38          17.33           184.60   \n",
      "1                 0.05667  ...         24.99          23.41           158.80   \n",
      "2                 0.05999  ...         23.57          25.53           152.50   \n",
      "3                 0.09744  ...         14.91          26.50            98.87   \n",
      "4                 0.05883  ...         22.54          16.67           152.20   \n",
      "5                 0.07613  ...         15.47          23.75           103.40   \n",
      "6                 0.05742  ...         22.88          27.66           153.20   \n",
      "7                 0.07451  ...         17.06          28.14           110.60   \n",
      "8                 0.07389  ...         15.49          30.73           106.20   \n",
      "9                 0.08243  ...         15.09          40.68            97.65   \n",
      "\n",
      "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "5       741.6            0.1791             0.5249           0.5355   \n",
      "6      1606.0            0.1442             0.2576           0.3784   \n",
      "7       897.0            0.1654             0.3682           0.2678   \n",
      "8       739.3            0.1703             0.5401           0.5390   \n",
      "9       711.4            0.1853             1.0580           1.1050   \n",
      "\n",
      "   concave_points_worst  symmetry_worst  fractal_dimension_worst  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "5                0.1741          0.3985                  0.12440  \n",
      "6                0.1932          0.3063                  0.08368  \n",
      "7                0.1556          0.3196                  0.11510  \n",
      "8                0.2060          0.4378                  0.10720  \n",
      "9                0.2210          0.4366                  0.20750  \n",
      "\n",
      "[10 rows x 30 columns]\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "5    1\n",
      "6    1\n",
      "7    1\n",
      "8    1\n",
      "9    1\n",
      "Name: diagnosis, dtype: int64\n",
      "input_dataset: 569\t output_dataset: 569\n"
     ]
    }
   ],
   "source": [
    "#define columns\n",
    "columns = [\n",
    "    'id', 'diagnosis', \n",
    "    'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean',\n",
    "    'compactness_mean', 'concavity_mean', 'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "    'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "    'compactness_se', 'concavity_se', 'concave_points_se', 'symmetry_se', 'fractal_dimension_se',\n",
    "    'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "    'compactness_worst', 'concavity_worst', 'concave_points_worst', 'symmetry_worst', 'fractal_dimension_worst'\n",
    "]\n",
    "column_types= ['categorical','categorical','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous','continuous',]\n",
    "base_dir = os.getcwd()\n",
    "data_path = os.path.join(base_dir, 'dataset', 'Breast_Cancer_Dataset', 'wdbc.data')\n",
    "df = pd.read_csv(data_path, header=None, names=columns)\n",
    "column_types.pop(df.columns.get_loc('id'))\n",
    "df.drop(columns='id', inplace=True)\n",
    "\n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "column_types.pop(df.columns.get_loc('diagnosis'))\n",
    "input_dataset=df.drop(columns='diagnosis')\n",
    "output_dataset= df['diagnosis']\n",
    "preview_input = input_dataset.head(10).copy()\n",
    "preview_output = output_dataset.head(10).copy()\n",
    "print(preview_input)\n",
    "print(preview_output)\n",
    "\n",
    "print(\"input_dataset: \"+ str(len(input_dataset)) + \"\\t output_dataset: \"+ str(len(output_dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c932efd3-55b9-4d42-bf17-f4e6537d699e",
   "metadata": {},
   "source": [
    "### Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2dcb31-9555-40e1-bb15-1df494e1730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for NaN or incompatible data entries and replacing the missing inputs with 0 and deleting the entries that lack an output\n",
    "if input_dataset.isnull().values.any():\n",
    "    input_dataset = input_dataset.fillna(0) #Filling missing inputs or NaN with 0\n",
    "valid_mask = output_dataset.isin([0, 1])\n",
    "if not valid_mask.all():\n",
    "    input_dataset = input_dataset[valid_mask].reset_index(drop=True) # dropping rows that lacks valid output\n",
    "    output_dataset = output_dataset[valid_mask].reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fdfc7a-324a-4451-b77e-7f9570e8f0e1",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "339003f2-e0af-4a4d-8d60-b8d02504dba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
      "0     1.096100     -2.071512        1.268817   0.983510         1.567087   \n",
      "1     1.828212     -0.353322        1.684473   1.907030        -0.826235   \n",
      "2     1.578499      0.455786        1.565126   1.557513         0.941382   \n",
      "3    -0.768233      0.253509       -0.592166  -0.763792         3.280667   \n",
      "4     1.748758     -1.150804        1.775011   1.824624         0.280125   \n",
      "5    -0.475956     -0.834601       -0.386808  -0.505206         2.235455   \n",
      "6     1.169878      0.160508        1.137124   1.094332        -0.123028   \n",
      "7    -0.118413      0.358135       -0.072803  -0.218772         1.602639   \n",
      "8    -0.319885      0.588312       -0.183919  -0.383870         2.199903   \n",
      "9    -0.473118      1.104467       -0.329192  -0.508616         1.581308   \n",
      "\n",
      "   compactness_mean  concavity_mean  concave_points_mean  symmetry_mean  \\\n",
      "0          3.280628        2.650542             2.530249       2.215566   \n",
      "1         -0.486643       -0.023825             0.547662       0.001391   \n",
      "2          1.052000        1.362280             2.035440       0.938859   \n",
      "3          3.399917        1.914213             1.450431       2.864862   \n",
      "4          0.538866        1.369806             1.427237      -0.009552   \n",
      "5          1.243242        0.865540             0.823931       1.004518   \n",
      "6          0.088218        0.299809             0.646366      -0.064268   \n",
      "7          1.139100        0.060972             0.281702       1.402121   \n",
      "8          1.682529        1.218025             1.149680       1.963872   \n",
      "9          2.561105        1.737343             0.940932       0.796597   \n",
      "\n",
      "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
      "0                2.253764  ...      1.885031      -1.358098         2.301575   \n",
      "1               -0.867889  ...      1.804340      -0.368879         1.533776   \n",
      "2               -0.397658  ...      1.510541      -0.023953         1.346291   \n",
      "3                4.906602  ...     -0.281217       0.133866        -0.249720   \n",
      "4               -0.561956  ...      1.297434      -1.465481         1.337363   \n",
      "5                1.888343  ...     -0.165353      -0.313560        -0.114908   \n",
      "6               -0.761662  ...      1.367780       0.322599         1.367122   \n",
      "7                1.658894  ...      0.163619       0.400695         0.099361   \n",
      "8                1.571079  ...     -0.161215       0.822090        -0.031581   \n",
      "9                2.780649  ...     -0.243975       2.440961        -0.286026   \n",
      "\n",
      "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
      "0    1.999478          1.306537           2.614365         2.107672   \n",
      "1    1.888827         -0.375282          -0.430066        -0.146620   \n",
      "2    1.455004          0.526944           1.081980         0.854222   \n",
      "3   -0.549538          3.391291           3.889975         1.987839   \n",
      "4    1.219651          0.220362          -0.313119         0.612640   \n",
      "5   -0.244105          2.046712           1.720103         1.262133   \n",
      "6    1.274098          0.518184           0.021196         0.509104   \n",
      "7    0.028834          1.446688           0.724148        -0.021035   \n",
      "8   -0.248145          1.661295           1.816711         1.278909   \n",
      "9   -0.297148          2.318256           5.108382         3.991920   \n",
      "\n",
      "   concave_points_worst  symmetry_worst  fractal_dimension_worst  \n",
      "0              2.294058        2.748204                 1.935312  \n",
      "1              1.086129       -0.243675                 0.280943  \n",
      "2              1.953282        1.151242                 0.201214  \n",
      "3              2.173873        6.040726                 4.930672  \n",
      "4              0.728618       -0.867590                -0.396751  \n",
      "5              0.905091        1.752527                 2.239831  \n",
      "6              1.195664        0.262245                -0.014718  \n",
      "7              0.623647        0.477221                 1.724917  \n",
      "8              1.390393        2.387756                 1.287517  \n",
      "9              1.618591        2.368360                 6.840837  \n",
      "\n",
      "[10 rows x 30 columns]\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "5    1\n",
      "6    1\n",
      "7    1\n",
      "8    1\n",
      "9    1\n",
      "Name: diagnosis, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Now instead of MinMax Scaler I would like to use NormalScaler without a library which is fairly easy\n",
    "input_mean=input_dataset.mean(axis=0) #axis=0 tells the function to operate columnwise\n",
    "input_std=input_dataset.std(axis=0)\n",
    "input_dataset_normalized = (input_dataset-input_mean)/(input_std + 0.000000000000000000001) #the constant value is to avoid division by zero because some of the datasets actually have a constant column so that std turns out to be 0\n",
    "\n",
    "preview_input = input_dataset_normalized.head(10).copy()\n",
    "preview_output = output_dataset.head(10).copy()\n",
    "print(preview_input)\n",
    "print(preview_output) \n",
    "#Let's see how the datasets have been prepared for further ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d0d7ed-8692-4be6-ad5c-e1cbaf42ec40",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19fde261-b03f-4171-bf45-70d7aa77fb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0     1     2     3     4     5     6     7     8     9   ...    20  \\\n",
      "0 -0.47 -0.16 -0.45 -0.49  0.23  0.03 -0.11 -0.28  0.41  0.13  ... -0.27   \n",
      "1  1.37  0.47  1.30  1.35 -0.45 -0.03  0.24  0.79 -0.84 -1.16  ...  1.78   \n",
      "2  0.38  0.04  0.40  0.27  0.91  0.34  0.73  0.82  0.44 -0.69  ...  0.62   \n",
      "3 -0.49 -0.37 -0.43 -0.53  0.64  0.52 -0.14 -0.54 -0.00  1.16  ... -0.70   \n",
      "4 -0.73 -1.13 -0.71 -0.72  0.25  0.15 -0.27 -0.59  0.02  0.71  ... -0.83   \n",
      "5  1.84  2.33  1.98  1.73  1.52  3.27  3.29  2.66  2.14  1.04  ...  1.96   \n",
      "6  2.24  0.61  2.27  2.35  0.71  1.72  1.96  2.61  0.05 -0.20  ...  2.36   \n",
      "7  0.98 -0.99  0.95  0.85  0.15  0.22  0.12  0.79 -0.26 -0.19  ...  0.77   \n",
      "8 -0.22 -0.80 -0.23 -0.38  0.81  0.93  0.35  0.54  0.48  0.88  ... -0.15   \n",
      "9 -0.06 -0.62 -0.12 -0.16 -2.00 -0.97 -0.83 -0.92  0.01 -1.05  ... -0.23   \n",
      "\n",
      "     21    22    23    24    25    26    27    28    29  \n",
      "0 -0.17 -0.33 -0.36  0.45 -0.10 -0.02 -0.20  0.18  0.20  \n",
      "1  0.15  1.75  1.73 -0.57 -0.13 -0.02  0.98 -0.57 -1.00  \n",
      "2  0.05  0.52  0.48  0.97 -0.09  0.51  0.56 -0.10 -0.21  \n",
      "3 -0.45 -0.53 -0.64  0.55  0.05 -0.15 -0.62 -0.56  0.53  \n",
      "4 -0.98 -0.85 -0.74  0.09 -0.27 -0.44 -0.69 -0.92 -0.14  \n",
      "5  2.24  2.30  1.65  1.43  3.90  3.19  2.29  1.92  2.22  \n",
      "6  0.02  2.61  2.36 -0.13  0.85  0.98  1.96 -0.26  0.10  \n",
      "7 -1.00  0.82  0.61 -0.30  0.17 -0.11  0.47 -0.23 -0.26  \n",
      "8 -0.40 -0.32 -0.47  0.93  1.43  1.02  0.85  1.01  0.98  \n",
      "9 -0.63 -0.18 -0.28 -1.69 -0.34 -0.64 -0.80 -0.36 -0.39  \n",
      "\n",
      "[10 rows x 30 columns]\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "5    1\n",
      "6    1\n",
      "7    1\n",
      "8    0\n",
      "9    0\n",
      "dtype: int64\n",
      "     0     1     2     3     4     5     6     7     8     9   ...    20  \\\n",
      "0  1.23  0.61  1.16  1.19 -0.15 -0.14  0.33  0.50 -0.44 -0.78  ...  1.29   \n",
      "1  1.27  0.22  1.24  1.25 -0.14  0.04  0.76  0.73 -0.42 -0.82  ...  1.04   \n",
      "2 -0.75 -1.09 -0.74 -0.71  0.59 -0.42 -0.45 -0.75 -0.12  0.42  ... -0.80   \n",
      "3  0.83 -0.05  0.88  0.68  1.26  1.00  1.28  1.55  1.17  0.06  ...  0.69   \n",
      "4  1.75  1.81  1.68  1.80  0.26  0.08  0.79  1.14  0.01 -1.00  ...  1.67   \n",
      "5 -0.13 -0.96 -0.15 -0.21 -0.97 -0.55 -0.58 -0.62 -0.07 -0.54  ... -0.36   \n",
      "6 -0.81  0.16 -0.75 -0.74 -1.15  0.26  0.05  0.18  2.86 -0.07  ... -0.91   \n",
      "7 -0.41 -1.66 -0.46 -0.45 -0.61 -0.88 -0.82 -0.64 -0.83 -0.01  ... -0.54   \n",
      "8  1.55 -0.26  1.59  1.59  1.11  1.18  2.03  2.05  0.79 -0.28  ...  2.16   \n",
      "9 -1.53 -0.57 -1.51 -1.20  0.54 -0.57 -1.11 -1.26  0.63  1.16  ... -1.26   \n",
      "\n",
      "     21    22    23    24    25    26    27    28    29  \n",
      "0  0.93  1.14  1.25  0.62 -0.17  0.60  0.35  0.34 -0.43  \n",
      "1  0.26  0.97  0.92  0.06 -0.27  0.35  0.52 -0.90 -0.54  \n",
      "2 -0.61 -0.75 -0.73  0.12 -0.34 -0.06 -0.61  0.07  0.43  \n",
      "3 -0.13  0.78  0.54  1.66  0.88  1.10  2.13  0.34  0.37  \n",
      "4  2.19  1.64  1.69  0.87  0.26  0.51  0.84  0.40 -0.22  \n",
      "5 -0.98 -0.28 -0.39 -0.21  0.36 -0.07 -0.14  0.79  0.69  \n",
      "6 -0.54 -0.86 -0.78 -1.30 -0.44 -0.56 -0.33  0.09 -0.77  \n",
      "7 -1.42 -0.57 -0.55 -0.04 -0.59 -0.85 -0.49 -0.20  0.59  \n",
      "8  0.12  2.01  2.37  0.50  0.83  1.96  1.67  1.20  0.46  \n",
      "9 -0.47 -1.29 -0.99  0.60 -0.78 -1.30 -1.74 -0.74  0.26  \n",
      "\n",
      "[10 rows x 30 columns]\n",
      "0    1\n",
      "1    1\n",
      "2    0\n",
      "3    1\n",
      "4    1\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    1\n",
      "9    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Split the dataset for 80-20 ratio, use 80% for training and 20% for testing, kind of like rule of thumb in ML.\n",
    "combined = list(zip(input_dataset_normalized.values.tolist(), output_dataset))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(combined)\n",
    "\n",
    "input_normalized_shuffled, output_shuffled = zip(*combined)\n",
    "input_normalized_shuffled = np.array(input_normalized_shuffled)\n",
    "output_shuffled = np.array(output_shuffled)\n",
    "split_point = int(0.8 * len(input_normalized_shuffled))\n",
    "input_normalized_shuffled_train_dataset, input_normalized_shuffled_test_dataset = input_normalized_shuffled[:split_point], input_normalized_shuffled[split_point:]\n",
    "output_shuffled_train_dataset, output_shuffled_test_dataset = output_shuffled[:split_point], output_shuffled[split_point:]\n",
    "\n",
    "\n",
    "preview_input_train = input_normalized_shuffled_train_dataset[:10]\n",
    "preview_output_train = output_shuffled_train_dataset[:10]\n",
    "preview_input_test = input_normalized_shuffled_test_dataset[:10]\n",
    "preview_output_test = output_shuffled_test_dataset[:10]\n",
    "\n",
    "print(pd.DataFrame(preview_input_train).round(2))\n",
    "print(pd.Series(preview_output_train))\n",
    "print(pd.DataFrame(preview_input_test).round(2))\n",
    "print(pd.Series(preview_output_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b927b837-7f5b-4468-8893-3654f8f5ab53",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b40fde60-f8e3-448a-b370-75bd904f3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.001, iter=1000, features=None, target_attribute=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iter = iter\n",
    "        self.features = features\n",
    "        self.target_attribute = target_attribute\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def train(self):\n",
    "        num_samples = self.features.shape[0]\n",
    "        num_features = self.features.shape[1]\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0.0\n",
    "\n",
    "        for _ in range(self.iter):\n",
    "            predicted_output = np.dot(self.features, self.weights) + self.bias\n",
    "            error = predicted_output - self.target_attribute\n",
    "            dw = np.mean(self.features * error[:, np.newaxis], axis=0)\n",
    "            db = np.mean(error)\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, input_test):\n",
    "        return np.dot(input_test, self.weights) + self.bias\n",
    "\n",
    "    def calculate_mse(self, input_test, actual_output):\n",
    "        predicted_output = self.predict(input_test)\n",
    "        mse = np.mean((predicted_output - actual_output) ** 2)\n",
    "        print(f\"Linear Regression MSE: {mse:.4f}\")\n",
    "        return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "524cc818-9a1c-40ca-a6d9-585d00b3f925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 0.1100\n",
      "Rounded Linear Regression Classification Accuracy: 87.72%\n"
     ]
    }
   ],
   "source": [
    "linreg = LinearRegression(\n",
    "    features=input_normalized_shuffled_train_dataset,\n",
    "    target_attribute=output_shuffled_train_dataset\n",
    ")\n",
    "linreg.train()\n",
    "lin_mse=linreg.calculate_mse(input_normalized_shuffled_test_dataset, output_shuffled_test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "output_pred = linreg.predict(input_normalized_shuffled_test_dataset)\n",
    "output_pred_rounded = np.round(output_pred).astype(int)\n",
    "\n",
    "correct_predictions = 0\n",
    "total_samples = len(output_shuffled_test_dataset)\n",
    "\n",
    "for i in range(total_samples):\n",
    "    predicted = output_pred_rounded[i]\n",
    "    actual = output_shuffled_test_dataset[i]\n",
    "    if predicted == actual:\n",
    "        correct_predictions += 1\n",
    "\n",
    "lin_accuracy = correct_predictions / total_samples\n",
    "print(f\"Rounded Linear Regression Classification Accuracy: {lin_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9738f2a-c5a6-4878-b5fc-ebd3752a9a34",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7418270-7151-4901-b559-758476df7b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 96.49%\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression is used because the output is categorized not a continous variable so no linear regression was used. \n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, iter=2000, features = None, target_attribute=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iter = iter\n",
    "        self.features = features\n",
    "        self.target_attribute = target_attribute\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def sigmoid_function(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def train(self):\n",
    "        num_samples = self.features.shape[0]\n",
    "        num_features = self.features.shape[1]\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0.0\n",
    "        \n",
    "        for _ in range(self.iter):\n",
    "            z = np.dot(self.features, self.weights) + self.bias\n",
    "            y_hat = self.sigmoid_function(z)\n",
    "            \n",
    "            error = (y_hat - self.target_attribute)\n",
    "            dw = np.mean(self.features * error[:, np.newaxis], axis=0)\n",
    "            db = np.mean(error)\n",
    "            \n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, input_test):  # Rename to match usage\n",
    "        z = np.dot(input_test, self.weights) + self.bias\n",
    "        probs = self.sigmoid_function(z)\n",
    "        return (probs >= 0.5).astype(int)\n",
    "\n",
    "logreg = CustomLogisticRegression(features = input_normalized_shuffled_train_dataset, target_attribute=output_shuffled_train_dataset)\n",
    "logreg.train()\n",
    "output_pred = logreg.predict(input_normalized_shuffled_test_dataset)\n",
    "log_accuracy = np.mean(output_pred == output_shuffled_test_dataset)\n",
    "print(f\"Logistic Regression Accuracy: {log_accuracy * 100:.2f}%\")\n",
    "# Predict the test set values and compute the accuracy of the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a44bfa2-43c7-45e2-a8ed-1940cbcfe0f4",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f1a9c84-b87a-4537-b1a4-6e222f65a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Initialize dictionaries to store class probabilities as well as the variance relative to the mean\n",
    "class BayessianClassifier:\n",
    "    def __init__(self, features, target_attribute):\n",
    "        self.features = features\n",
    "        self.target_attribute = target_attribute\n",
    "        self.available_classes = np.unique(self.target_attribute)\n",
    "        \n",
    "        self.mean = {}\n",
    "        self.var = {}\n",
    "        self.class_probabilities={}\n",
    "        \n",
    "    def get_probabilistic_values(self):\n",
    "         # Calculate the probability, mean and variance\n",
    "        total_samples = len(self.target_attribute)\n",
    "        for cls in self.available_classes:\n",
    "            # Select the data subset\n",
    "            input_occurence= self.features[self.target_attribute== cls]\n",
    "            self.class_probabilities[cls]=len(input_occurence)/total_samples\n",
    "            self.mean[cls] = np.mean(input_occurence, axis=0)\n",
    "            self.var[cls] = np.var(input_occurence, axis=0)\n",
    "\n",
    "    # Gaussian Probability Density Function\n",
    "    def predict(self, data_entry):\n",
    "        log_posterior_array=[]\n",
    "        for cls in self.available_classes:\n",
    "            log_likelihood=0\n",
    "            for i, col in enumerate(data_entry):\n",
    "                log_likelihood=log_likelihood+((-((col-self.mean[cls][i]) ** 2))/(2*self.var[cls][i]+1e-6))+np.log(1/(np.sqrt(2*np.pi*self.var[cls][i]+1e-6)))\n",
    "            log_posterior = log_likelihood + np.log(self.class_probabilities[cls])\n",
    "            log_posterior_array.append(log_posterior)    \n",
    "        return self.available_classes[np.argmax(log_posterior_array)]\n",
    "    \n",
    "\n",
    "    def calculate_efficiency(self,test_input,test_output):\n",
    "        correct_predictions=0\n",
    "        for i in range(len(test_input)):\n",
    "            predicted = self.predict(test_input[i])\n",
    "            actual = test_output[i]\n",
    "            if predicted == actual:\n",
    "                correct_predictions += 1\n",
    "        accuracy = correct_predictions / len(test_input)\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "673935e8-3e6e-44d5-8165-d36a2081b82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.98%\n"
     ]
    }
   ],
   "source": [
    "# Train the Gaussian Naive Bayes classifier\n",
    "model = BayessianClassifier(features=input_normalized_shuffled_train_dataset, target_attribute=output_shuffled_train_dataset)\n",
    "model.get_probabilistic_values()\n",
    "naive_bayes_accuracy=model.calculate_efficiency(input_normalized_shuffled_test_dataset, output_shuffled_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44075a18-01cd-43a4-85ad-fc20b2d323e1",
   "metadata": {},
   "source": [
    "### KNearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "050e85c9-0636-468b-99a9-b7629a445132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_if_closer(neighbors, candidate):\n",
    "    \"\"\"\n",
    "    neighbors: list of (label, distance)\n",
    "    candidate: a new tuple (label, distance)\n",
    "    \"\"\"\n",
    "    max_dist = -float('inf')\n",
    "    max_index = -1\n",
    "\n",
    "    for i in range(len(neighbors)):\n",
    "        if neighbors[i][1] > max_dist:\n",
    "            max_dist = neighbors[i][1]\n",
    "            max_index = i\n",
    "\n",
    "    if candidate[1] < max_dist:\n",
    "        neighbors[max_index] = candidate\n",
    "    \n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=3, features=None, target_attribute=None):\n",
    "        self.k = k\n",
    "        self.features = features\n",
    "        self.target_attribute = target_attribute\n",
    "\n",
    "    def get_closest_neighbours(self, data_entry):\n",
    "        # Start with dummy neighbors with infinite distance\n",
    "        closest_neighbours = [(None, float('inf')) for _ in range(self.k)]\n",
    "\n",
    "        for i in range(len(self.features)):\n",
    "            distance = np.sqrt(np.sum((self.features[i] - data_entry) ** 2))\n",
    "            label = self.target_attribute[i]\n",
    "            candidate = (label, distance)\n",
    "\n",
    "            replace_if_closer(closest_neighbours, candidate)\n",
    "        return closest_neighbours\n",
    "\n",
    "    def predict(self, closest_neighbours):\n",
    "        label_counts = {}\n",
    "    \n",
    "        for label, _ in closest_neighbours:\n",
    "            if label in label_counts:\n",
    "                label_counts[label] += 1\n",
    "            else:\n",
    "                label_counts[label] = 1\n",
    "    \n",
    "        # Find the label with the highest count\n",
    "        max_count = -1\n",
    "        predicted_label = None\n",
    "    \n",
    "        for label in label_counts:\n",
    "            if label_counts[label] > max_count:\n",
    "                max_count = label_counts[label]\n",
    "                predicted_label = label\n",
    "    \n",
    "        return predicted_label\n",
    "    \n",
    "    def calculate_efficiency(self, test_input, test_output):\n",
    "        correct = 0\n",
    "        total_samples = len(test_input)\n",
    "    \n",
    "        for i in range(total_samples):\n",
    "            data_entry = test_input[i]\n",
    "            true_label = test_output[i]\n",
    "    \n",
    "            # Step 1: Get k nearest neighbors\n",
    "            closest_neighbors = self.get_closest_neighbours(data_entry)\n",
    "    \n",
    "            # Step 2: Predict label based on majority vote\n",
    "            predicted_label = self.predict(closest_neighbors)\n",
    "    \n",
    "            # Step 3: Compare with actual label\n",
    "            if predicted_label == true_label:\n",
    "                correct += 1\n",
    "    \n",
    "        # Step 4: Compute accuracy\n",
    "        accuracy = correct / total_samples\n",
    "        print(f\"KNNClassifier Accuracy: {accuracy * 100:.2f}%\")\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a03fdb-4d90-4583-8785-b2e1c57ddbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNNClassifier Accuracy: 94.74%\n"
     ]
    }
   ],
   "source": [
    "model = KNNClassifier(features=input_normalized_shuffled_train_dataset, target_attribute=output_shuffled_train_dataset)\n",
    "KNN_accuracy=model.calculate_efficiency(input_normalized_shuffled_test_dataset, output_shuffled_test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2b4331-4659-47d9-8c35-0d59f08aa97d",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74b525a0-22c0-48eb-b8bc-f40d18a04c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, features, target_attribute, column_types, max_depth):\n",
    "        self.depth=None\n",
    "        self.features = features\n",
    "        self.target_attribute = target_attribute\n",
    "        self.column_types = column_types\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "        if features is not None and column_types is not None:\n",
    "            if len(column_types) != features.shape[1]:\n",
    "                raise ValueError(\"Length of column_types must match number of feature columns\")\n",
    "\n",
    "    def calculate_entropy(self, output_column):\n",
    "        values, counts = np.unique(output_column, return_counts=True) #get the unique class labels from output column with their corresponding occurence frequency.\n",
    "        probabilities = counts / len(output_column) #array of probabilities by dividing occurence array by the number of rows. \n",
    "        entropy = -np.sum([p * np.log2(p + 1e-9) for p in probabilities]) #calculate the given samples entropy\n",
    "        return entropy\n",
    "\n",
    "    def categorical_split_entropy(self, feature_column, output_column):\n",
    "        unique_values = np.unique(feature_column)\n",
    "        total_entropy = 0\n",
    "        for val in unique_values:\n",
    "            mask = feature_column == val #boolean array of where the categorical value selected inside the feature column for the current iteration, return true if matches in array false if it doesn't match\n",
    "            y_sub = output_column[mask] # clipping the output column by only showing True returned booleans from the mask\n",
    "            entropy = self.calculate_entropy(y_sub) # calculate the entropy for the specific attribute chosen for this iteration in the specific feature chosen in this function\n",
    "            weight = len(y_sub) / len(output_column) #calculate the weight of the specific attribute for this iteration\n",
    "            total_entropy += weight * entropy #multiply specific attribute of the specific feature with its weight, iterate through each attribute for the specific feature eventually obtain the entropy for that specific feature column, will be used later for comparison\n",
    "        return total_entropy\n",
    "\n",
    "    def numerical_split_entropy(self, feature_column, output_column):\n",
    "        sorted_indices = np.argsort(feature_column) #sort the indices chosen feature columns numerical attributes\n",
    "        feature_column = feature_column[sorted_indices] #sort the feature column according to the sorted indices above\n",
    "        output_column = output_column[sorted_indices]  #sort the output column according to the sorted indices above\n",
    "    \n",
    "        best_entropy = float('inf')\n",
    "        best_threshold = None\n",
    "        base_entropy = self.calculate_entropy(output_column) #get the parent entropy\n",
    "    \n",
    "        for i in range(len(feature_column) - 1):\n",
    "            threshold = (feature_column[i] + feature_column[i + 1]) / 2\n",
    "    \n",
    "            left_mask = feature_column <= threshold #getting the mask boolean array into two categories, numerical attribute lower than threshold and numerical attribute greater than threshold\n",
    "            right_mask = feature_column > threshold\n",
    "    \n",
    "            left_output = output_column[left_mask] #getting the array itself according to the boolean array created above\n",
    "            right_output = output_column[right_mask]\n",
    "    \n",
    "            if len(left_output) == 0 or len(right_output) == 0: #skip these iterations if there are duplicate data right at the border and split simply split data into 0 and remaining, meaning split didn't actually split anything at all\n",
    "                continue\n",
    "    \n",
    "            left_entropy = self.calculate_entropy(left_output) #calculate entropy for left side\n",
    "            right_entropy = self.calculate_entropy(right_output) #calculate entropy for right side\n",
    "    \n",
    "            weighted_entropy = (len(left_output) * left_entropy + len(right_output) * right_entropy) / len(output_column) #get the split weighted entropy sum\n",
    "    \n",
    "            if weighted_entropy < best_entropy: #if split weighted entropy sum is lower than the best one so far, this is the best one and this splti poitn is the best split point\n",
    "                best_entropy = weighted_entropy\n",
    "                best_threshold = threshold\n",
    "    \n",
    "        info_gain = base_entropy - best_entropy #calculate info gain according to the best weighted split entropy  \n",
    "        return best_entropy, best_threshold, info_gain\n",
    "\n",
    "    def find_best_split(self, features=None, target=None, depth=0):\n",
    "        if features is None:\n",
    "            features = self.features\n",
    "        if target is None:\n",
    "            target = self.target_attribute\n",
    "\n",
    "        if len(np.unique(target)) == 1: #if the split has only one possible output, conclude that leaf with the given output\n",
    "            return {'label': target[0]}\n",
    "\n",
    "        if len(target) <= 5:   #if the split has fewer rows than 5, take the maximum occurence of the output class as reference and conclude the leaf\n",
    "            return {'label': np.bincount(target).argmax()}\n",
    "\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return {'label': np.bincount(target).argmax()}\n",
    "\n",
    "        base_entropy = self.calculate_entropy(target)\n",
    "        best_gain = -float('inf')\n",
    "        best_column = None\n",
    "        best_type = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for col_index in range(features.shape[1]):\n",
    "            feature_col = features[:, col_index] #extract the col\n",
    "            col_type = self.column_types[col_index] #get the col type\n",
    "\n",
    "            if col_type == 'categorical': #if the col is categorical, apply categorical entropy calculation\n",
    "                entropy = self.categorical_split_entropy(feature_col, target)\n",
    "                threshold = None\n",
    "                info_gain = base_entropy - entropy\n",
    "            else:                          #if the col is numerical, apply numerical entropy calculation\n",
    "                entropy, threshold, info_gain = self.numerical_split_entropy(feature_col, target)\n",
    "\n",
    "            if info_gain > best_gain: #get the best info gain\n",
    "                best_gain = info_gain\n",
    "                best_column = col_index\n",
    "                best_type = col_type\n",
    "                best_threshold = threshold\n",
    "\n",
    "        return best_column, best_type, best_threshold, best_gain\n",
    "\n",
    "    def build_tree(self, features=None, target=None, depth=0):\n",
    "        if features is None:\n",
    "            features = self.features\n",
    "        if target is None:\n",
    "            target = self.target_attribute\n",
    "\n",
    "        split = self.find_best_split(features, target, depth)\n",
    "        if isinstance(split, dict) and 'label' in split: #looks if split returned a leaf node or a split\n",
    "            return split\n",
    "\n",
    "        best_col, col_type, threshold, _ = split\n",
    "        feature_col = features[:, best_col]\n",
    "\n",
    "        if col_type == 'categorical':\n",
    "            branches = {}\n",
    "            for val in np.unique(feature_col):\n",
    "                mask = feature_col == val\n",
    "                sub_features = features[mask]\n",
    "                sub_target = target[mask]\n",
    "                branches[val] = self.build_tree(sub_features, sub_target, depth + 1)\n",
    "            return {\n",
    "                'feature_index': best_col,\n",
    "                'type': col_type,\n",
    "                'branches': branches\n",
    "            }\n",
    "        else:\n",
    "            left_mask = feature_col <= threshold\n",
    "            right_mask = feature_col > threshold\n",
    "\n",
    "            left_features, left_target = features[left_mask], target[left_mask]\n",
    "            right_features, right_target = features[right_mask], target[right_mask]\n",
    "            return {\n",
    "                'feature_index': best_col,\n",
    "                'type': col_type,\n",
    "                'threshold': threshold,\n",
    "                'left': self.build_tree(left_features, left_target, depth + 1),\n",
    "                'right': self.build_tree(right_features, right_target, depth + 1)\n",
    "            }\n",
    "\n",
    "    def train(self):\n",
    "        self.tree = self.build_tree()\n",
    "        self.depth = self.get_tree_depth(self.tree)  \n",
    "\n",
    "    def get_tree_depth(self, node):\n",
    "        if 'label' in node:\n",
    "            return 0\n",
    "        return 1 + max(self.get_tree_depth(node['left']), self.get_tree_depth(node['right']))\n",
    "\n",
    "    def predict_one(self, input_row, tree):\n",
    "        if 'label' in tree:\n",
    "            return tree['label']\n",
    "\n",
    "        if tree['type'] == 'categorical':\n",
    "            val = input_row[tree['feature_index']]\n",
    "            if val in tree['branches']:\n",
    "                return self.predict_one(input_row, tree['branches'][val])\n",
    "            else:\n",
    "                return list(tree['branches'].values())[0]['label']\n",
    "        else:\n",
    "            if input_row[tree['feature_index']] <= tree['threshold']:\n",
    "                return self.predict_one(input_row, tree['left'])\n",
    "            else:\n",
    "                return self.predict_one(input_row, tree['right'])\n",
    "\n",
    "    def predict(self, input_test):\n",
    "        return np.array([self.predict_one(row, self.tree) for row in input_test])\n",
    "\n",
    "    def calculate_efficiency(self, test_input, test_output):\n",
    "        predictions = self.predict(test_input)\n",
    "        accuracy = np.mean(predictions == test_output)\n",
    "        print(f\"Decision Tree Classification Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab409824-6a8b-4468-a56c-c62f25206a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth Tried: 1, Actual Depth: 1, Accuracy: 85.96%\n",
      "Max Depth Tried: 2, Actual Depth: 2, Accuracy: 85.96%\n",
      "Max Depth Tried: 3, Actual Depth: 3, Accuracy: 92.11%\n",
      "Max Depth Tried: 4, Actual Depth: 4, Accuracy: 91.23%\n",
      "Max Depth Tried: 5, Actual Depth: 5, Accuracy: 94.74%\n",
      "Max Depth Tried: 6, Actual Depth: 5, Accuracy: 94.74%\n",
      "\n",
      "Max Possible Depth is: 6\n",
      "\n",
      "Best Depth: 5 with Accuracy: 94.74%\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "best_depth = 1\n",
    "depth = 1\n",
    "max_possible_depth=None\n",
    "\n",
    "while True:\n",
    "    tree_clf = DecisionTree(\n",
    "        features=input_normalized_shuffled_train_dataset,\n",
    "        target_attribute=output_shuffled_train_dataset,\n",
    "        column_types=column_types,\n",
    "        max_depth=depth\n",
    "    )\n",
    "    tree_clf.train()\n",
    "    predictions = tree_clf.predict(input_normalized_shuffled_test_dataset)\n",
    "    accuracy = np.mean(predictions == output_shuffled_test_dataset)\n",
    "    \n",
    "    print(f\"Max Depth Tried: {depth}, Actual Depth: {tree_clf.depth}, Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_depth = depth\n",
    "\n",
    "    # If increasing depth no longer increases actual tree depth, break\n",
    "    if tree_clf.depth < depth:\n",
    "        max_possible_depth=depth\n",
    "        break\n",
    "\n",
    "    depth += 1\n",
    "\n",
    "print(f\"\\nMax Possible Depth is: {max_possible_depth}\")\n",
    "print(f\"\\nBest Depth: {best_depth} with Accuracy: {best_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4919ff7c-d8ba-44a8-a083-c59d2c37f621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Mean Square Error:  0.10997694540102625\n",
      "Linear Regression Rounded Accuracy Percentage:  0.8771929824561403\n",
      "Logistic Regression Accuracy: 96.49%\n",
      "Decision Tree Classification Accuracy: 94.74%\n",
      "KNN Classification Accuracy: 94.74%\n",
      "Gaussian Naive Bayes Accuracy: 92.98%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Linear Regression Mean Square Error: \", lin_mse)\n",
    "print(\"Linear Regression Rounded Accuracy Percentage: \", lin_accuracy)\n",
    "print(f\"Logistic Regression Accuracy: {log_accuracy * 100:.2f}%\")\n",
    "print(f\"Decision Tree Classification Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "print(f\"KNN Classification Accuracy: {KNN_accuracy * 100:.2f}%\")\n",
    "print(f\"Gaussian Naive Bayes Accuracy: {naive_bayes_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
