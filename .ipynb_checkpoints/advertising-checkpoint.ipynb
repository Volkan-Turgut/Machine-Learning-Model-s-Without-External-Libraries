{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1292f5-69d2-40c5-a1a4-a86ee1098b15",
   "metadata": {},
   "source": [
    "# Advertising Data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "982c9d51-0180-4678-a7d9-b50b35a445ca",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/tawfikelmetwally/advertising-dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9d26b42-498f-4cc4-9109-58b3cdfc6b3f",
   "metadata": {},
   "source": [
    "This data expresses sales according to the type of advertisement and the size of the cost .\n",
    "The dataset contains 200 rows of 3 features [ TV , Radio , Newspaper] and target variable [Sales]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf881d-efac-4de6-a1c3-827c167a171f",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300e8bd8-2b10-4421-a4bf-a7b532fd11dd",
   "metadata": {},
   "source": [
    "#### Prepping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f31fbb-a0e0-4ff9-a899-0051eee257e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LinearRegression:\n",
    "    # initialize a learning rate and parameters\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
    "        # the learning rate will update and the number of iterations for training increaes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    # training the model to fit on a line of best fit \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            # Calculate predicted values\n",
    "            y_predicted = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # Calculate gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    # predict the info \n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "        \n",
    "    # Mean Squared Error - evaluate the models performane \n",
    "    def mean_squared_error(self, y_true, y_pred):\n",
    "        # Calculate the MSE by averaging \n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e6af2-ce10-48cc-bebc-3a66bd8db38c",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d17dbf20-4aa3-480c-a0b8-3fc7c65608cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "        \n",
    "    # apply the sigmoid function to the linear combination\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    # fit the logistic regression model\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Training Loop - update weights and bias\n",
    "        for _ in range(self.epochs):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        return [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "    \n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        return np.mean(y_true == y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4b578b9-974e-4ca0-9e1f-71bda7bce2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 3.175711859938966\n",
      "Logistic Regression Accuracy: 0.925\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the dataset\n",
    "def load_data():\n",
    "    data = pd.read_csv(\"Advertising.csv\")\n",
    "    X = data[[\"TV\", \"Radio\", \"Newspaper\"]].values\n",
    "    y = data[\"Sales\"].values\n",
    "\n",
    "    \n",
    "    # Normalize features (optional but recommended)\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    \n",
    "    # Convert sales into binary classes for logistic regression\n",
    "    y_logistic = (y > np.median(y)).astype(int)\n",
    "    \n",
    "    # Manually split into train and test sets\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    test_size = int(0.2 * len(X))\n",
    "    \n",
    "    # Split  linear regression\n",
    "    X_train = X[indices[test_size:]]\n",
    "    X_test = X[indices[:test_size]]\n",
    "    y_train = y[indices[test_size:]]\n",
    "    y_test = y[indices[:test_size]]\n",
    "    \n",
    "    # Split logistic regression\n",
    "    X_train_log = X[indices[test_size:]]\n",
    "    X_test_log = X[indices[:test_size]]\n",
    "    y_train_log = y_logistic[indices[test_size:]]\n",
    "    y_test_log = y_logistic[indices[:test_size]]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, X_train_log, X_test_log, y_train_log, y_test_log\n",
    "\n",
    "# Train and evaluate models\n",
    "X_train, X_test, y_train, y_test, X_train_log, X_test_log, y_train_log, y_test_log = load_data()\n",
    "\n",
    "# Linear Regression Model\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X_train, y_train)\n",
    "y_pred = linear_regression.predict(X_test)\n",
    "mse = linear_regression.mean_squared_error(y_test, y_pred)\n",
    "print(f\"Linear Regression MSE: {mse}\")\n",
    "\n",
    "# Logistic Regression Model\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train_log, y_train_log)\n",
    "y_pred_log = logistic_regression.predict(X_test_log)\n",
    "accuracy = logistic_regression.accuracy(y_test_log, y_pred_log)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32624476-2b6c-42b5-8528-b206ab742639",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "174c990f-e27b-4ed9-a67d-0809bb97ad4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy k=3: 0.26695776243494773\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Advertising.csv')\n",
    "\n",
    "# Standardize the features (TV, Radio, Newspaper) using Z-score standardization\n",
    "def standardize_data(data):\n",
    "    return (data - data.mean()) / data.std()\n",
    "\n",
    "    # Standarize the data \n",
    "\n",
    "df['TV'] = standardize_data(df['TV'])\n",
    "df['Radio'] = standardize_data(df['Radio'])\n",
    "df['Newspaper'] = standardize_data(df['Newspaper'])\n",
    "\n",
    "# Step 3: Split the data into training and testing sets (80-20 split)\n",
    "def train_test_split(data, test_size=0.2):\n",
    "    # Shuffle the data\n",
    "    data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    test_data_size = int(len(data) * test_size)\n",
    "\n",
    "    # Feature and the target \n",
    "    X_train = data.iloc[:-test_data_size, :-1].values  \n",
    "    y_train = data.iloc[:-test_data_size, -1].values    \n",
    "    X_test = data.iloc[-test_data_size:, :-1].values  \n",
    "    y_test = data.iloc[-test_data_size:, -1].values   \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df)\n",
    "\n",
    "\n",
    "# Function to compute Euclidean distance between two points\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "# Function to predict the target value for a given test set\n",
    "def knn_predict(X_train, y_train, X_test, k=5):\n",
    "    y_pred = []\n",
    "    \n",
    "    for test_point in X_test:\n",
    "        # Calculate distance between test_point and all training points\n",
    "        distances = [euclidean_distance(test_point, train_point) for train_point in X_train]\n",
    "        \n",
    "        # Get indices of the k nearest neighbors\n",
    "        # Get the labels (Sales) of the k nearest neighbors\n",
    "        k_neighbors_indices = np.argsort(distances)[:k]        \n",
    "        k_nearest_labels = y_train[k_neighbors_indices]\n",
    "\n",
    "        # Predict by averaging the target values of the k nearest neighbors\n",
    "        prediction = np.mean(k_nearest_labels)\n",
    "        y_pred.append(prediction)\n",
    "        \n",
    "    return np.array(y_pred)\n",
    "\n",
    "# Calculate RÂ² (R-squared) to evaluate accuracy\n",
    "def r_squared(y_true, y_pred):\n",
    "    # Residual sum of squares\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2\n",
    "\n",
    "# Try different k values\n",
    "k = 3\n",
    "y_pred = knn_predict(X_train, y_train, X_test, k)\n",
    "r2 = r_squared(y_test, y_pred)\n",
    "print(f'KNN Accuracy k={k}: {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feaaa26-669b-43e6-9364-19efec0ceed6",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8330ebdd-2494-472c-93ae-51019298eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Decision Tree (Accuracy): 0.8765723773254737\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Advertising.csv')\n",
    "\n",
    "# Standardize the features\n",
    "def standardize_data(data):\n",
    "    return (data - data.mean()) / data.std()\n",
    "\n",
    "df['TV'] = standardize_data(df['TV'])\n",
    "df['Radio'] = standardize_data(df['Radio'])\n",
    "df['Newspaper'] = standardize_data(df['Newspaper'])\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = df[['TV', 'Radio', 'Newspaper']].values\n",
    "y = df['Sales'].values\n",
    "\n",
    "# Train-Test Split function (80-20 split)\n",
    "def train_test_split(X, y, test_size=0.2):\n",
    "    # Shuffle the dataset\n",
    "    indices = np.random.permutation(len(X))\n",
    "    test_size = int(len(X) * test_size)\n",
    "    \n",
    "    train_indices = indices[test_size:]\n",
    "    test_indices = indices[:test_size]\n",
    "    \n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Gini Impurity function\n",
    "def gini_impurity(y):\n",
    "    classes = np.unique(y)\n",
    "    impurity = 1\n",
    "    for c in classes:\n",
    "        prob_of_c = np.sum(y == c) / len(y)\n",
    "        impurity -= prob_of_c ** 2\n",
    "    return impurity\n",
    "\n",
    "# Split function based on a feature and threshold\n",
    "def split_data(X, y, feature_index, threshold):\n",
    "    left_mask = X[:, feature_index] <= threshold\n",
    "    right_mask = ~left_mask\n",
    "    X_left, y_left = X[left_mask], y[left_mask]\n",
    "    X_right, y_right = X[right_mask], y[right_mask]\n",
    "    return X_left, y_left, X_right, y_right\n",
    "\n",
    "# Decision Tree Node class\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "        # fit \n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_classes = np.unique(y)\n",
    "        \n",
    "        # Stopping condition: Pure node or max depth or too few samples\n",
    "        if len(unique_classes) == 1 or depth == self.max_depth or num_samples < self.min_samples_split:\n",
    "            return np.mean(y) \n",
    "        \n",
    "        best_gini = float('inf')\n",
    "        best_split = None\n",
    "        best_left = None\n",
    "        best_right = None\n",
    "        \n",
    "        # Iterate over all features and thresholds to find the best split\n",
    "        for feature_index in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                X_left, y_left, X_right, y_right = split_data(X, y, feature_index, threshold)\n",
    "                gini = (len(X_left) / len(X)) * gini_impurity(y_left) + (len(X_right) / len(X)) * gini_impurity(y_right)\n",
    "                \n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_split = (feature_index, threshold)\n",
    "                    best_left, best_right = (X_left, y_left), (X_right, y_right)\n",
    "\n",
    "        # Create subtrees \n",
    "        left_node = self._build_tree(best_left[0], best_left[1], depth + 1)\n",
    "        right_node = self._build_tree(best_right[0], best_right[1], depth + 1)\n",
    "        \n",
    "        return (best_split, left_node, right_node)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict_sample(x, self.tree) for x in X]\n",
    "\n",
    "    def _predict_sample(self, x, node):\n",
    "        if isinstance(node, (int, float)):\n",
    "            return node\n",
    "        feature_index, threshold = node[0]\n",
    "        if x[feature_index] <= threshold:\n",
    "            return self._predict_sample(x, node[1])\n",
    "        else:\n",
    "            return self._predict_sample(x, node[2])\n",
    "\n",
    "# R-Squared Accuracy function\n",
    "def r_squared(y_true, y_pred):\n",
    "    total_variance = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    residual_variance = np.sum((y_true - y_pred) ** 2)\n",
    "    return 1 - (residual_variance / total_variance)\n",
    "\n",
    "# Train the decision tree model\n",
    "tree = DecisionTree(max_depth=5, min_samples_split=10)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = tree.predict(X_test)\n",
    "r2 = r_squared(y_test, y_pred)\n",
    "print(f' Decision Tree (Accuracy): {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88d5a0-e0db-423f-ba03-df5186b9e558",
   "metadata": {},
   "source": [
    "### Bayesian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf88903f-b72b-411e-8804-d2dda62b01d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Classifier Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Bayesian Classifier \n",
    "class BayesianClassifier:\n",
    "    def __init__(self):\n",
    "        self.means = {}\n",
    "        self.variances = {}\n",
    "        self.priors = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.means[c] = np.mean(X_c, axis=0)\n",
    "            self.variances[c] = np.var(X_c, axis=0)\n",
    "            self.priors[c] = X_c.shape[0] / X.shape[0]\n",
    "\n",
    "    # Create Predicition\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            posteriors = []\n",
    "            for c in self.classes:\n",
    "                prior = np.log(self.priors[c])\n",
    "                likelihood = -0.5 * np.sum(np.log(2 * np.pi * self.variances[c])) - 0.5 * np.sum(((x - self.means[c]) ** 2) / (self.variances[c]))\n",
    "                posteriors.append(prior + likelihood)\n",
    "            predictions.append(self.classes[np.argmax(posteriors)])\n",
    "        return predictions\n",
    "    \n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        return np.mean(y_true == y_pred)\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_data():\n",
    "    data = pd.read_csv(\"Advertising.csv\")\n",
    "    X = data[[\"TV\", \"Radio\", \"Newspaper\"]].values\n",
    "    y = data[\"Sales\"].values\n",
    "    \n",
    "    # Normalize features (optional but recommended)\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    \n",
    "    # Convert sales into binary classes \n",
    "    y_classification = (y > np.median(y)).astype(int)\n",
    "    \n",
    "    # Manually split into train and test sets\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    test_size = int(0.2 * len(X))\n",
    "    \n",
    "    # Split for Bayesian classifier\n",
    "    X_train_class = X[indices[test_size:]]\n",
    "    X_test_class = X[indices[:test_size]]\n",
    "    y_train_class = y_classification[indices[test_size:]]\n",
    "    y_test_class = y_classification[indices[:test_size]]\n",
    "    \n",
    "    return X_train_class, X_test_class, y_train_class, y_test_class\n",
    "\n",
    "# Train and evaluate Bayesian Classifier model\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = load_data()\n",
    "\n",
    "# Bayesian Classifier Model\n",
    "bayesian_classifier = BayesianClassifier()\n",
    "bayesian_classifier.fit(X_train_class, y_train_class)\n",
    "y_pred_bayes = bayesian_classifier.predict(X_test_class)\n",
    "accuracy_bayes = bayesian_classifier.accuracy(y_test_class, y_pred_bayes)\n",
    "print(f\"Bayesian Classifier Accuracy: {accuracy_bayes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9abaef-85b8-4479-abba-86aa152ded8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
